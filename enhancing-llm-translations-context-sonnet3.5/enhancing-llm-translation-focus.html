<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Enhancing LLM Translation Focus</title>
    
    <!-- Tailwind CSS Play CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="../styles.css">
    <script>
        tailwind.config = {
            darkMode: 'class',
        }
    </script>
</head>
<body class="bg-white dark:bg-slate-900 text-slate-800 dark:text-slate-200 transition-colors">
    
    <!-- Progress Bar -->
    <div class="progress-bar fixed top-0 left-0 h-1 bg-blue-500 z-50 transition-all duration-100"></div>
    
    <!-- Reading Time Display -->
    <div id="readingTimeDisplay" class="fixed top-2 left-4 px-3 py-1 bg-slate-100 dark:bg-slate-800 border border-slate-300 dark:border-slate-600 rounded-full text-xs text-slate-600 dark:text-slate-400 z-50 transition-colors">
        <span id="readingTimeText">üìñ Calculating...</span>
    </div>
    
    <!-- Dark Mode Toggle -->
    <button id="themeToggle" class="fixed top-4 right-4 w-12 h-12 rounded-full bg-slate-100 dark:bg-slate-800 border border-slate-300 dark:border-slate-600 hover:bg-slate-200 dark:hover:bg-slate-700 flex items-center justify-center text-xl z-50 transition-colors">
        <span id="themeIcon">üåô</span>
    </button>
    
    <!-- Table of Contents -->
    <div class="toc-container fixed top-0 right-0 h-screen w-72 bg-slate-50 dark:bg-slate-800 border-l border-slate-200 dark:border-slate-700 z-40 overflow-y-auto">
        <div class="absolute -left-10 top-1/2 -translate-y-1/2 w-10 h-15 bg-slate-50 dark:bg-slate-800 border border-r-0 border-slate-200 dark:border-slate-700 rounded-l-lg flex items-center justify-center cursor-pointer text-slate-600 dark:text-slate-400">
            ‚ò∞
        </div>
        <div class="p-4">
            <h3 class="text-lg font-bold mb-4 text-slate-800 dark:text-slate-200">Contents</h3>
            <nav id="tocNav" class="space-y-2">
                <!-- TOC will be generated by JavaScript -->
            </nav>
        </div>
    </div>
    
    <!-- Main Content -->
    <main class="max-w-none mx-auto px-6 py-8" style="max-width: 70ch;">
        
        <!-- Prefix Panel -->
        <section class="bg-blue-50 dark:bg-blue-950 border-l-4 border-blue-400 dark:border-blue-500 p-4 rounded-r-lg mb-8">
            <h2 class="section-header text-xl font-semibold mb-3 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">üìã How This Document Was Created</h2>
            <div class="section-content">
                <p class="text-sm text-slate-600 dark:text-slate-400 mb-2">
                    This document represents a condensed version of a longer work, transformed into an interactive web format. Here's what was retained, condensed, or omitted:
                </p>
                <ul class="text-sm text-slate-600 dark:text-slate-400 space-y-1 ml-4">
                    <li><strong>Retained:</strong> Core arguments, key insights, and main conclusions</li>
                    <li><strong>Condensed:</strong> Complex details simplified for broader accessibility</li>
                    <li><strong>Omitted:</strong> Repetitive sections, extensive citations, and tangential content</li>
                </ul>
            </div>
        </section>
        
        <!-- Document Summary -->
        <section class="mb-8">
            <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">üìÑ Summary</h2>
            <div class="section-content">
                <p class="text-lg leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    This paper proposes a novel method called Dynamic Focus Anchoring (DFA) to enhance Large Language Models' machine translation capabilities. The method addresses the challenge of translating context-sensitive units (CSUs) like polysemous words by dynamically analyzing and identifying translation challenges, then incorporating them into LLMs in a structured manner.
                </p>
                <p class="text-lg leading-relaxed text-slate-700 dark:text-slate-300">
                    The proposed method achieves competitive performance compared to existing baseline models across multiple language pairs, without requiring additional model training or parallel data. The approach demonstrates effectiveness in both similar language pairs (e.g., English-German) and distant language pairs (e.g., English-Chinese).
                </p>
            </div>
        </section>
        
        <!-- Content Sections -->
        <section class="mb-8">
            <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">1. Introduction</h2>
            <div class="section-content">
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    The rapid development of large language models (LLMs) has revolutionized cross-lingual NLP tasks, particularly in machine translation (MT). Current LLM-based translation approaches primarily use two methods:
                </p>
                
                <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
                    <li>Prompt engineering</li>
                    <li>Instruction fine-tuning</li>
                </ul>
                
                <div class="bg-amber-50 dark:bg-amber-950 border-l-4 border-amber-400 dark:border-amber-500 p-4 rounded-r-lg mb-4">
                    <p class="text-sm text-amber-800 dark:text-amber-200">
                        <strong>Key Challenge:</strong> While these methods show impressive capabilities, they face persistent challenges in translating context-sensitive units (CSUs), such as polysemous words. These CSUs can impair the model's understanding of the entire sentence and may even lead to translation failure.
                    </p>
                </div>

                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    The key issue lies in how LLMs handle knowledge utilization for CSUs, which can lead to semantic ambiguity. Most LLM-based MT methods treat the entire sentence as a homogeneous unit, neglecting that different words have varying levels of translation difficulty. For example, polysemous words require contextual understanding for accurate translation.
                </p>

                <div class="bg-blue-50 dark:bg-blue-950 border-l-4 border-blue-400 dark:border-blue-500 p-4 rounded-r-lg mb-4">
                    <p class="text-sm text-blue-800 dark:text-blue-200">
                        <strong>Our Solution:</strong> We propose a resource-efficient method that integrates semantic focus into dynamic structured prompts. This approach helps overcome the knowledge extraction bottleneck of LLMs through two main stages:
                    </p>
                    <ul class="list-disc list-inside space-y-1 text-sm text-blue-800 dark:text-blue-200 mt-2">
                        <li>CSU identification and classification</li>
                        <li>Hierarchical semantic constraint injection</li>
                    </ul>
                </div>

                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Key Contributions</h3>
                <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
                    <li>First investigation of CSUs' impact on LLM's cross-lingual translation, identifying the "semantic ambiguity" problem</li>
                    <li>Novel and resource-efficient method to enhance LLMs' MT capabilities using semantic focus in dynamic structured prompts</li>
                    <li>Extensive experiments showing significant improvement in translation accuracy across various language pairs</li>
                </ul>
            </div>
        </section>
        
        <!-- Add more sections as we process them -->
        
        <!-- Glossary -->
        <section class="mb-8">
            <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">üìö Glossary</h2>
            <div class="section-content">
                <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                    <div class="bg-slate-50 dark:bg-slate-800 p-3 rounded-lg border border-slate-200 dark:border-slate-700">
                        <h4 class="font-semibold text-slate-800 dark:text-slate-200">CSU (Context-Sensitive Unit)</h4>
                        <p class="text-sm text-slate-600 dark:text-slate-400">Words with complex semantics or words that are uncommon to the model, such as polysemous words, domain-specific terms, and culturally specific vocabulary.</p>
                    </div>
                    <div class="bg-slate-50 dark:bg-slate-800 p-3 rounded-lg border border-slate-200 dark:border-slate-700">
                        <h4 class="font-semibold text-slate-800 dark:text-slate-200">DFA (Dynamic Focus Anchoring)</h4>
                        <p class="text-sm text-slate-600 dark:text-slate-400">A method that enhances LLM translation by identifying CSUs and injecting semantic focus into the translation prompt.</p>
                    </div>
                </div>
            </div>
        </section>
        
        <section class="mb-8">
            <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">2. Preliminary Experiment</h2>
            <div class="section-content">
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    To understand the impact of CSUs in LLMs' machine translation task, a series of experiments were conducted using English sentences containing polysemous words (words with multiple possible translations in Chinese).
                </p>

                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Baseline Test</h3>
                <div class="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 mb-4">
                    <p class="text-sm mb-2"><strong>Example Sentence:</strong> "The bank can be very dangerous this time of year."</p>
                    <ul class="text-sm space-y-1">
                        <li><strong>Baseline Translation:</strong> "‰ªäÂπ¥Ëøô‰∏™Êó∂ÂÄôÔºåÈì∂Ë°åÂæàÂç±Èô©„ÄÇ" (incorrect - translated as "financial bank")</li>
                        <li><strong>Correct Translation:</strong> "‰ªäÂπ¥Ëøô‰∏™Êó∂ÂÄôÔºåÊ≤≥Â≤∏ÂæàÂç±Èô©„ÄÇ" (correct - translated as "river bank")</li>
                    </ul>
                </div>

                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">CSU-Enhanced Test</h3>
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    The same prompt template was tested with the addition of CSU indicators: "Note: the following should be translated carefully + CSUs." This modification led to significant improvements:
                </p>

                <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
                    <div class="bg-green-50 dark:bg-green-950 p-4 rounded-lg">
                        <h4 class="font-semibold text-green-800 dark:text-green-200 mb-2">Improvement 1: Better Accuracy</h4>
                        <p class="text-sm text-green-700 dark:text-green-300">By highlighting CSUs, the model provided more accurate translations for difficult words.</p>
                    </div>
                    <div class="bg-green-50 dark:bg-green-950 p-4 rounded-lg">
                        <h4 class="font-semibold text-green-800 dark:text-green-200 mb-2">Improvement 2: Reduced Failures</h4>
                        <p class="text-sm text-green-700 dark:text-green-300">Pointing out challenging parts helped prevent translation failures and improved overall sentence understanding.</p>
                    </div>
                </div>

                <div class="bg-amber-50 dark:bg-amber-950 border-l-4 border-amber-400 dark:border-amber-500 p-4 rounded-r-lg mb-4">
                    <p class="text-sm text-amber-800 dark:text-amber-200">
                        <strong>Important Finding:</strong> When too much information was provided (like including all possible translations), it had a counterproductive effect. The model became reliant on given translations rather than performing active analysis.
                    </p>
                </div>

                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Key Conclusions</h3>
                <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
                    <li>Semantic ambiguities can cause LLMs to misinterpret CSUs in context</li>
                    <li>Poor handling of CSUs can affect the entire sentence translation</li>
                    <li>Simply providing reference translations is not effective - the model needs guidance for independent analysis</li>
                </ul>
            </div>
        </section>
        
        <section class="mb-8">
            <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">3. Methodology</h2>
            <div class="section-content">
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    Based on the preliminary experiments, we propose a simple yet effective method to enhance LLMs' machine translation capabilities by explicitly indicating CSUs to provide meta-cognitive guidance. The method addresses the "semantic ambiguity" phenomenon and provides a framework to improve overall translation performance.
                </p>

                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">3.1 Semantic Confusion</h3>
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    Current prompt-based MT paradigms for LLMs show critical brittleness when processing sentences containing CSUs. Our analysis revealed that this is due to semantic confusion in handling context-sensitive units.
                </p>

                <div class="bg-blue-50 dark:bg-blue-950 border-l-4 border-blue-400 dark:border-blue-500 p-4 rounded-r-lg mb-4">
                    <p class="text-sm text-blue-800 dark:text-blue-200">
                        <strong>Definition of CSUs:</strong> Words with complex semantics or words that are uncommon to the model, such as:
                    </p>
                    <ul class="list-disc list-inside space-y-1 text-sm text-blue-800 dark:text-blue-200 mt-2">
                        <li>Polysemous words</li>
                        <li>Domain-specific terms</li>
                        <li>Culturally specific vocabulary</li>
                    </ul>
                </div>

                <h4 class="text-md font-semibold mb-2 text-slate-800 dark:text-slate-200">Key Characteristics of CSUs</h4>
                <div class="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 mb-4">
                    <ul class="space-y-2 text-sm">
                        <li><strong>Context Sensitivity:</strong> CSUs are highly sensitive to context and may present significantly different semantic meanings as the context varies.</li>
                        <li><strong>Translation Complexity:</strong> The accurate translation of CSUs requires precise understanding of the context and proper selection from multiple possible translations.</li>
                        <li><strong>Impact on Understanding:</strong> Mishandling CSUs can affect not just the local translation but the model's understanding of the entire sentence.</li>
                    </ul>
                </div>

                <div class="bg-amber-50 dark:bg-amber-950 border-l-4 border-amber-400 dark:border-amber-500 p-4 rounded-r-lg mb-4">
                    <p class="text-sm text-amber-800 dark:text-amber-200">
                        <strong>Consequences of Semantic Confusion:</strong>
                    </p>
                    <ul class="list-disc list-inside space-y-1 text-sm text-amber-800 dark:text-amber-200 mt-2">
                        <li>Incorrect translations of CSUs due to failure to grasp true meaning in context</li>
                        <li>Impaired understanding of entire sentences</li>
                        <li>Translation failures (non-task responses or gibberish outputs)</li>
                    </ul>
                </div>

                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    This intrinsic ambiguity sensitivity fundamentally undermines conventional prompt engineering strategies that naively position CSUs within static phrasal templates. This observation led to the development of our Dynamic Focus Anchoring (DFA) method, which we'll discuss in the next section.
                </p>

                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">3.2 Dynamic Focus Anchoring (DFA) Method</h3>
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    Drawing upon our observations of the semantic ambiguity problem, we developed the Dynamic Focus Anchoring (DFA) method. This approach consists of two main components: CSUs identification and semantic focus injection.
                </p>

                <h4 class="text-md font-semibold mb-2 text-slate-800 dark:text-slate-200">CSUs Identification</h4>
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    The detection of CSUs is fundamental to our method. To address the scarcity of semantic confusion datasets, we developed a dual-layer semantic exploration mechanism:
                </p>

                <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
                    <div class="bg-purple-50 dark:bg-purple-950 p-4 rounded-lg">
                        <h5 class="font-semibold text-purple-800 dark:text-purple-200 mb-2">External Knowledge Acquisition</h5>
                        <p class="text-sm text-purple-700 dark:text-purple-300">Uses multilingual lexicons (MUSE dataset) to identify polysemous words, with semantic filtering through word embedding clustering.</p>
                    </div>
                    <div class="bg-purple-50 dark:bg-purple-950 p-4 rounded-lg">
                        <h5 class="font-semibold text-purple-800 dark:text-purple-200 mb-2">Internal Knowledge Activation</h5>
                        <p class="text-sm text-purple-700 dark:text-purple-300">Leverages the model's internal knowledge to identify domain-specific terms and culturally unique vocabulary.</p>
                    </div>
                </div>

                <div class="bg-blue-50 dark:bg-blue-950 border-l-4 border-blue-400 dark:border-blue-500 p-4 rounded-r-lg mb-4">
                    <p class="text-sm text-blue-800 dark:text-blue-200">
                        <strong>Semantic Filter Process:</strong>
                    </p>
                    <ol class="list-decimal list-inside space-y-1 text-sm text-blue-800 dark:text-blue-200 mt-2">
                        <li>Identify potential polysemous words using multilingual lexicons</li>
                        <li>Cluster word embeddings of translations</li>
                        <li>Words with multiple semantic clusters are marked as CSUs</li>
                        <li>Single-cluster words are filtered out</li>
                    </ol>
                </div>

                <h4 class="text-md font-semibold mb-2 text-slate-800 dark:text-slate-200">Semantic Focus Injection</h4>
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    After identifying CSUs, we integrate them into the foundational MT prompt through semantic focus injection. This process follows two main principles:
                </p>

                <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
                    <div class="bg-green-50 dark:bg-green-950 p-4 rounded-lg">
                        <h5 class="font-semibold text-green-800 dark:text-green-200 mb-2">Initiative Guidance</h5>
                        <p class="text-sm text-green-700 dark:text-green-300">Avoids providing direct translations, instead triggers the model's internal knowledge inspection.</p>
                    </div>
                    <div class="bg-green-50 dark:bg-green-950 p-4 rounded-lg">
                        <h5 class="font-semibold text-green-800 dark:text-green-200 mb-2">Knowledge Resource Optimization</h5>
                        <p class="text-sm text-green-700 dark:text-green-300">Focuses attention on key words through lexical semantics, reducing attention to irrelevant parts.</p>
                    </div>
                </div>

                <div class="bg-amber-50 dark:bg-amber-950 border-l-4 border-amber-400 dark:border-amber-500 p-4 rounded-r-lg mb-4">
                    <p class="text-sm text-amber-800 dark:text-amber-200">
                        <strong>Implementation Note:</strong> To maintain prompt efficiency, we limit the number of CSUs to k (optimally set to 8 based on experiments). This prevents instruction length from interfering with the LLM's processing capabilities.
                    </p>
                </div>

                <h4 class="text-md font-semibold mb-2 text-slate-800 dark:text-slate-200">Enhanced Prompt Structure</h4>
                <div class="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 mb-4">
                    <p class="text-sm mb-2"><strong>Format:</strong></p>
                    <pre class="text-sm bg-slate-100 dark:bg-slate-900 p-2 rounded">
I_enhanced = I_base ‚äï W_CSUs
where:
- I_base = Basic translation instruction
- W_CSUs = Set of identified CSUs
- ‚äï = Focus injection operation</pre>
                </div>
            </div>
        </section>
        
        <section class="mb-8">
            <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">4. Experimental Setup</h2>
            <div class="section-content">
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    We conducted comprehensive experiments to evaluate the effectiveness of our proposed DFA method. The experiments followed standard MT setup practices from previous work and covered both similar and distant language pairs.
                </p>

                <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
                    <div class="bg-blue-50 dark:bg-blue-950 p-4 rounded-lg">
                        <h4 class="font-semibold text-blue-800 dark:text-blue-200 mb-2">Language Pairs Tested</h4>
                        <ul class="list-disc list-inside text-sm text-blue-700 dark:text-blue-300">
                            <li>Similar: English-German (EN-DE)</li>
                            <li>Distant: English-Chinese (EN-ZH)</li>
                        </ul>
                    </div>
                    <div class="bg-blue-50 dark:bg-blue-950 p-4 rounded-lg">
                        <h4 class="font-semibold text-blue-800 dark:text-blue-200 mb-2">Datasets</h4>
                        <ul class="list-disc list-inside text-sm text-blue-700 dark:text-blue-300">
                            <li>WMT22 test set</li>
                            <li>MUSE lexicon for CSUs</li>
                        </ul>
                    </div>
                </div>

                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Model Configuration</h3>
                <div class="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 mb-4">
                    <h4 class="font-semibold mb-2">Backbone Models:</h4>
                    <ul class="space-y-2 text-sm">
                        <li><strong>Primary Models:</strong> Llama2-7b and Llama3-8b</li>
                        <li><strong>Max Text Length:</strong> 256 tokens</li>
                        <li><strong>Beam Search:</strong> 5 beams</li>
                        <li><strong>Hardware:</strong> Single Nvidia RTX A6000</li>
                    </ul>
                </div>

                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Evaluation Metrics</h3>
                <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mb-4">
                    <div class="bg-purple-50 dark:bg-purple-950 p-4 rounded-lg">
                        <h4 class="font-semibold text-purple-800 dark:text-purple-200 mb-2">BLEU Score</h4>
                        <p class="text-sm text-purple-700 dark:text-purple-300">Classic metric using Sacre implementation</p>
                    </div>
                    <div class="bg-purple-50 dark:bg-purple-950 p-4 rounded-lg">
                        <h4 class="font-semibold text-purple-800 dark:text-purple-200 mb-2">COMET Score</h4>
                        <p class="text-sm text-purple-700 dark:text-purple-300">Using wmt22-comet-da, optimized for LLM translations</p>
                    </div>
                    <div class="bg-purple-50 dark:bg-purple-950 p-4 rounded-lg">
                        <h4 class="font-semibold text-purple-800 dark:text-purple-200 mb-2">ChrF2</h4>
                        <p class="text-sm text-purple-700 dark:text-purple-300">Additional character-level metric</p>
                    </div>
                </div>

                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Baseline Models</h3>
                <div class="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 mb-4">
                    <ul class="space-y-2 text-sm">
                        <li><strong>SOTA Models:</strong>
                            <ul class="list-disc list-inside ml-4">
                                <li>ParroT</li>
                                <li>Bayling and Bayling2</li>
                                <li>TASTE (Fixemb-QE and Fixemb-TC variants)</li>
                            </ul>
                        </li>
                        <li><strong>Traditional Models:</strong>
                            <ul class="list-disc list-inside ml-4">
                                <li>MT-Full</li>
                                <li>MT-FixEmb</li>
                            </ul>
                        </li>
                    </ul>
                </div>

                <div class="bg-amber-50 dark:bg-amber-950 border-l-4 border-amber-400 dark:border-amber-500 p-4 rounded-r-lg">
                    <p class="text-sm text-amber-800 dark:text-amber-200">
                        <strong>Note:</strong> Our method does not require model training or parallel data, making it particularly efficient compared to traditional approaches that need extensive training resources.
                    </p>
                </div>
            </div>
        </section>
        
        <section class="mb-8">
            <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">5. Results and Discussion</h2>
            <div class="section-content">
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    This section empirically demonstrates the effectiveness of our method in the LLMs machine translation scenario, including main experiments, ablation studies, additional evaluations, and detailed analyses.
                </p>

                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">5.1 Main Results</h3>
                <div class="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 mb-4">
                    <p class="text-sm mb-2">The proposed DFA method shows significant improvements:</p>
                    <ul class="text-sm space-y-1">
                        <li>Average improvement of 0.83 in COMET scores</li>
                        <li>Average improvement of 0.81 in BLEU scores</li>
                        <li>No requirement for model fine-tuning or parallel sentence-level MT corpora</li>
                    </ul>
                </div>

                <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
                    <div class="bg-green-50 dark:bg-green-950 p-4 rounded-lg">
                        <h4 class="font-semibold text-green-800 dark:text-green-200 mb-2">Similar Language Pairs</h4>
                        <p class="text-sm text-green-700 dark:text-green-300">For EN-DE language pair with Llama2 backbone: 1.11 improvement in COMET scores and 0.22 in BLEU scores over Bayling2 baseline.</p>
                    </div>
                    <div class="bg-green-50 dark:bg-green-950 p-4 rounded-lg">
                        <h4 class="font-semibold text-green-800 dark:text-green-200 mb-2">Distant Language Pairs</h4>
                        <p class="text-sm text-green-700 dark:text-green-300">For EN-ZH language pair with Llama2 backbone: 0.3 improvement in COMET scores and 0.35 in BLEU scores over Bayling2 baseline.</p>
                    </div>
                </div>

                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">5.2 Ablation Study</h3>
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    The ablation study validated the effectiveness of CSU extraction and semantic focus by testing each component separately:
                </p>
                <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
                    <li>Each type of CSU (polysemous, domain-specific, cultural) contributed to accuracy improvement</li>
                    <li>Both external bilingual dictionaries and internal knowledge activation proved effective</li>
                    <li>The method successfully leverages both external data and LLM's internal knowledge</li>
                </ul>

                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">5.3 Other Evaluations</h3>
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    Additional evaluation metrics (chrF2, BLEU4) were used to validate the method's effectiveness:
                </p>
                <div class="bg-blue-50 dark:bg-blue-950 border-l-4 border-blue-400 dark:border-blue-500 p-4 rounded-r-lg mb-4">
                    <p class="text-sm text-blue-800 dark:text-blue-200">
                        Results showed consistent improvement across all metrics, confirming the robustness of the proposed method.
                    </p>
                </div>

                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">5.4 Analysis about Semantic Filter</h3>
                <div class="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 mb-4">
                    <p class="text-sm mb-2">Key findings from semantic filter analysis:</p>
                    <ul class="text-sm space-y-1">
                        <li>Semantic-filtered polysemous words provided better guidance for LLMs</li>
                        <li>Simple translation count-based selection decreased accuracy</li>
                        <li>Words with multiple translations but similar semantics were effectively filtered out</li>
                    </ul>
                </div>

                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">5.5 Analysis about CSUs Number</h3>
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    Analysis of the optimal number of CSUs in prompts revealed:
                </p>
                <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
                    <li>Both too few and too many CSUs led to suboptimal performance</li>
                    <li>Optimal parameter k=8 was determined through experiments</li>
                    <li>Maximum value without restrictions resulted in lower COMET scores</li>
                </ul>

                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">5.6 Case Study</h3>
                <div class="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 mb-4">
                    <p class="text-sm mb-2">Representative examples demonstrated:</p>
                    <ul class="text-sm space-y-1">
                        <li>DFA corrected translation failures for polysemous words</li>
                        <li>Improved accuracy in CSU translations</li>
                        <li>Better alignment with reference translations</li>
                    </ul>
                </div>
            </div>
        </section>
        
        <section class="mb-8">
            <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">6. Related Work</h2>
            <div class="section-content">
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    Current research on enhancing LLM-based machine translation primarily falls into two main categories:
                </p>

                <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
                    <div class="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700">
                        <h4 class="font-semibold text-slate-800 dark:text-slate-200 mb-2">1. Prompt Engineering</h4>
                        <ul class="text-sm space-y-2">
                            <li>Focuses on designing and optimizing prompt templates</li>
                            <li>Bayling utilizes bilingual parallel pairs for rare words</li>
                            <li>Few-shot prompting provides contextually similar translations</li>
                        </ul>
                    </div>
                    <div class="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700">
                        <h4 class="font-semibold text-slate-800 dark:text-slate-200 mb-2">2. Instruction Fine-tuning</h4>
                        <ul class="text-sm space-y-2">
                            <li>Enhances model's understanding of task-specific instructions</li>
                            <li>Requires model training resources and time</li>
                            <li>Produces results more aligned with human needs</li>
                        </ul>
                    </div>
                </div>

                <div class="bg-blue-50 dark:bg-blue-950 border-l-4 border-blue-400 dark:border-blue-500 p-4 rounded-r-lg mb-4">
                    <p class="text-sm text-blue-800 dark:text-blue-200">
                        <strong>Our Novel Approach:</strong> Unlike existing methods, our approach:
                    </p>
                    <ul class="list-disc list-inside space-y-1 text-sm text-blue-800 dark:text-blue-200 mt-2">
                        <li>Analyzes and classifies CSUs in sentences to be translated</li>
                        <li>Stimulates model to extract and utilize relevant knowledge</li>
                        <li>Requires no bilingual examples or additional training</li>
                    </ul>
                </div>

                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Key Differences from Existing Work</h3>
                <div class="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 mb-4">
                    <ul class="text-sm space-y-2">
                        <li><strong>Resource Efficiency:</strong> No need for parallel data or model training</li>
                        <li><strong>Focus Mechanism:</strong> Dynamic identification and handling of challenging vocabulary</li>
                        <li><strong>Flexibility:</strong> Works with both external data and internal knowledge</li>
                        <li><strong>Universal Applicability:</strong> Effective across different language pairs</li>
                    </ul>
                </div>
            </div>
        </section>
        
        <section class="mb-8">
            <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">7. Conclusion</h2>
            <div class="section-content">
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    In this study, we propose an effective method DFA to enhance LLM's MT ability based on dynamic focus anchoring and injecting. The semantic confusion problem is noticed, and is alleviated by analysis hard translated words (CSUs) and inject related focuses into LLMs.
                </p>

                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    Two ways to obtain the CSUs are provided, namely, from external lexical data and LLMs internal knowledge activation, both can be used flexibly in a variety of application scenarios. Extensive experiments demonstrate the effectiveness and robustness of DFA across diverse experimental languages.
                </p>

                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    Overall, our work presents a novel method for LLM-based MT, alleviating semantic confusion and offering promising results. Focusing on the CSUs helps LLMs to accurately retrieve the knowledge related to the translation focus and incorporate it into the translation process, leading to the generation of high-quality translations without model training.
                </p>
            </div>
        </section>
        
    </main>
    
    <script src="../script.js"></script>
</body>
</html> 