<!-- Comments Section -->
<h2 class="text-2xl font-bold mb-6 text-slate-800 dark:text-slate-200">ðŸ’¬ Community Discussion</h2>

<!-- Dr. Sarah Chen (Academic Researcher) -->
<div class="no-audio mb-6 bg-slate-50 dark:bg-slate-800 rounded-lg p-4 border border-slate-200 dark:border-slate-700">
    <div class="flex items-start space-x-3">
        <div class="flex-shrink-0">
            <div class="w-10 h-10 bg-blue-500 rounded-full flex items-center justify-center text-white font-semibold text-sm">
                SC
            </div>
        </div>
        <div class="flex-grow">
            <div class="flex items-center space-x-2 mb-2">
                <h4 class="font-semibold text-slate-800 dark:text-slate-200">Dr. Sarah Chen</h4>
                <span class="text-xs text-slate-500 dark:text-slate-400">CS Professor</span>
                <span class="text-xs text-slate-400">â€¢</span>
                <span class="text-xs text-slate-400">1d ago</span>
            </div>
            <p class="text-slate-700 dark:text-slate-300 leading-relaxed">
                The benchmark-driven approach here is rigorous and aligns with best practices in AI research. I appreciate the inclusion of both proprietary and open-source models. However, I would like to see more discussion on the reproducibility of these benchmarks and the impact of prompt engineering on reported results. Have you considered including error bars or confidence intervals for the performance metrics?
            </p>
        </div>
    </div>
</div>

<!-- Marcus Rodriguez (Industry Professional) -->
<div class="no-audio mb-6 bg-slate-50 dark:bg-slate-800 rounded-lg p-4 border border-slate-200 dark:border-slate-700">
    <div class="flex items-start space-x-3">
        <div class="flex-shrink-0">
            <div class="w-10 h-10 bg-green-500 rounded-full flex items-center justify-center text-white font-semibold text-sm">
                MR
            </div>
        </div>
        <div class="flex-grow">
            <div class="flex items-center space-x-2 mb-2">
                <h4 class="font-semibold text-slate-800 dark:text-slate-200">Marcus Rodriguez</h4>
                <span class="text-xs text-slate-500 dark:text-slate-400">Senior Engineer</span>
                <span class="text-xs text-slate-400">â€¢</span>
                <span class="text-xs text-slate-400">3h ago</span>
            </div>
            <p class="text-slate-700 dark:text-slate-300 leading-relaxed">
                As someone deploying LLMs in production, the cost/performance tradeoff tables are invaluable. I do wonder about the real-world latency and throughput for these models, especially under heavy load. Has anyone benchmarked these models in a live agentic workflow with concurrent users?
            </p>
        </div>
    </div>
</div>

<!-- Emma Thompson (Graduate Student) -->
<div class="no-audio mb-6 bg-slate-50 dark:bg-slate-800 rounded-lg p-4 border border-slate-200 dark:border-slate-700">
    <div class="flex items-start space-x-3">
        <div class="flex-shrink-0">
            <div class="w-10 h-10 bg-purple-500 rounded-full flex items-center justify-center text-white font-semibold text-sm">
                ET
            </div>
        </div>
        <div class="flex-grow">
            <div class="flex items-center space-x-2 mb-2">
                <h4 class="font-semibold text-slate-800 dark:text-slate-200">Emma Thompson</h4>
                <span class="text-xs text-slate-500 dark:text-slate-400">PhD Student</span>
                <span class="text-xs text-slate-400">â€¢</span>
                <span class="text-xs text-slate-400">5h ago</span>
            </div>
            <p class="text-slate-700 dark:text-slate-300 leading-relaxed">
                This comparison is a fantastic resource for academic research! I'm curious about the ethical implications of using proprietary versus open-source models in educational settings. How do you ensure transparency and fairness when recommending models for research or teaching?
            </p>
        </div>
    </div>
</div>

<!-- Alex Park (Skeptical Technologist) -->
<div class="no-audio mb-6 bg-slate-50 dark:bg-slate-800 rounded-lg p-4 border border-slate-200 dark:border-slate-700">
    <div class="flex items-start space-x-3">
        <div class="flex-shrink-0">
            <div class="w-10 h-10 bg-red-500 rounded-full flex items-center justify-center text-white font-semibold text-sm">
                AP
            </div>
        </div>
        <div class="flex-grow">
            <div class="flex items-center space-x-2 mb-2">
                <h4 class="font-semibold text-slate-800 dark:text-slate-200">Alex Park</h4>
                <span class="text-xs text-slate-500 dark:text-slate-400">Senior Developer</span>
                <span class="text-xs text-slate-400">â€¢</span>
                <span class="text-xs text-slate-400">7h ago</span>
            </div>
            <p class="text-slate-700 dark:text-slate-300 leading-relaxed">
                Impressive data, but I'm skeptical about the generalizability of these results. Benchmarks are great, but real-world edge cases often expose model weaknesses. What's the plan for ongoing evaluation as new models and benchmarks emerge?
            </p>
        </div>
    </div>
</div>

<!-- James Kim (Startup Founder) -->
<div class="no-audio mb-6 bg-slate-50 dark:bg-slate-800 rounded-lg p-4 border border-slate-200 dark:border-slate-700">
    <div class="flex items-start space-x-3">
        <div class="flex-shrink-0">
            <div class="w-10 h-10 bg-yellow-500 rounded-full flex items-center justify-center text-white font-semibold text-sm">
                JK
            </div>
        </div>
        <div class="flex-grow">
            <div class="flex items-center space-x-2 mb-2">
                <h4 class="font-semibold text-slate-800 dark:text-slate-200">James Kim</h4>
                <span class="text-xs text-slate-500 dark:text-slate-400">Startup Founder</span>
                <span class="text-xs text-slate-400">â€¢</span>
                <span class="text-xs text-slate-400">12h ago</span>
            </div>
            <p class="text-slate-700 dark:text-slate-300 leading-relaxed">
                This is the kind of analysis I wish I had when evaluating LLMs for our customer support platform. The recommendations section is especially helpful. Any thoughts on how these models perform for multilingual support or in markets with limited training data?
            </p>
        </div>
    </div>
</div>

<!-- "TruthSeeker42" (Conspiracy Theorist) -->
<div class="no-audio mb-6 bg-slate-50 dark:bg-slate-800 rounded-lg p-4 border border-slate-200 dark:border-slate-700">
    <div class="flex items-start space-x-3">
        <div class="flex-shrink-0">
            <div class="w-10 h-10 bg-gray-700 rounded-full flex items-center justify-center text-white font-semibold text-sm">
                TS
            </div>
        </div>
        <div class="flex-grow">
            <div class="flex items-center space-x-2 mb-2">
                <h4 class="font-semibold text-slate-800 dark:text-slate-200">TruthSeeker42</h4>
                <span class="text-xs text-slate-500 dark:text-slate-400">Conspiracy Theorist</span>
                <span class="text-xs text-slate-400">â€¢</span>
                <span class="text-xs text-slate-400">1d ago</span>
            </div>
            <p class="text-slate-700 dark:text-slate-300 leading-relaxed">
                WAKE UP! This isn't just about comparing AIsâ€”it's about CONTROL. Why do you think all these companies are racing to build bigger models? It's not for your benefit. They want to monitor every word, every thought. Project Blue Beam, anyone? Do your own research! #StayWoke
            </p>
        </div>
    </div>
</div> 