<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Enhancing Large Language Models' Machine Translation via Dynamic Focus Anchoring</title>
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../styles.css">
    
    <!-- Tailwind CSS Play CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            darkMode: 'class',
        }
    </script>

</head>
<body class="bg-white dark:bg-slate-900 text-slate-800 dark:text-slate-200 transition-colors">
    
    <!-- Progress Bar -->
    <div class="progress-bar fixed top-0 left-0 h-1 bg-blue-500 z-50 transition-all duration-100"></div>
    
    <!-- Reading Time Display -->
    <div id="readingTimeDisplay" class="fixed top-2 left-4 px-3 py-1 bg-slate-100 dark:bg-slate-800 border border-slate-300 dark:border-slate-600 rounded-full text-xs text-slate-600 dark:text-slate-400 z-50 transition-colors">
        <span id="readingTimeText">üìñ Calculating...</span>
    </div>
    
    <!-- Dark Mode Toggle -->
    <button id="themeToggle" class="fixed top-4 right-4 w-12 h-12 rounded-full bg-slate-100 dark:bg-slate-800 border border-slate-300 dark:border-slate-600 hover:bg-slate-200 dark:hover:bg-slate-700 flex items-center justify-center text-xl z-50 transition-colors">
        <span id="themeIcon">üåô</span>
    </button>
    
    <!-- Table of Contents -->
    <div class="toc-container fixed top-0 right-0 h-screen w-72 bg-slate-50 dark:bg-slate-800 border-l border-slate-200 dark:border-slate-700 z-40 overflow-y-auto">
        <div class="absolute -left-10 top-1/2 -translate-y-1/2 w-10 h-15 bg-slate-50 dark:bg-slate-800 border border-r-0 border-slate-200 dark:border-slate-700 rounded-l-lg flex items-center justify-center cursor-pointer text-slate-600 dark:text-slate-400">
            ‚ò∞
        </div>
        <div class="p-4">
            <h3 class="text-lg font-bold mb-4 text-slate-800 dark:text-slate-200">Contents</h3>
            <nav id="tocNav" class="space-y-2">
                <!-- TOC will be generated by JavaScript -->
            </nav>
        </div>
    </div>
    
    <!-- Main Content -->
    <main class="max-w-none mx-auto px-6 py-8" style="max-width: 70ch;">
        
        <!-- Prefix Panel -->
        <section class="bg-blue-50 dark:bg-blue-950 border-l-4 border-blue-400 dark:border-blue-500 p-4 rounded-r-lg mb-8">
            <h2 class="section-header text-xl font-semibold mb-3 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">üìã How This Document Was Created</h2>
            <div class="section-content">
                <p class="text-sm text-slate-600 dark:text-slate-400 mb-2">
                    This document represents a condensed version of a longer work, transformed into an interactive web format. Here's what was retained, condensed, or omitted:
                </p>
                <ul class="text-sm text-slate-600 dark:text-slate-400 space-y-1 ml-4">
                    <li><strong>Retained:</strong> Core arguments, key insights, and main conclusions</li>
                    <li><strong>Condensed:</strong> Complex details simplified for broader accessibility</li>
                    <li><strong>Omitted:</strong> Repetitive sections, extensive citations, and tangential content</li>
                </ul>
            </div>
        </section>
        
        <!-- Document Summary -->
        <section class="mb-8">
            <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">üìÑ Summary</h2>
            <div class="section-content">
                <p class="text-lg leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    Large language models have shown exceptional performance in machine translation, but they struggle with <span class="glossary-term relative border-b border-dotted border-slate-400 cursor-help" data-definition="Words with complex semantics that are highly sensitive to context, such as polysemous words">Context-Sensitive Units (CSUs)</span> like polysemous words. This paper introduces <span class="glossary-term relative border-b border-dotted border-slate-400 cursor-help" data-definition="A resource-efficient method that enhances LLM translation by dynamically identifying and focusing on challenging vocabulary">Dynamic Focus Anchoring (DFA)</span>, a simple yet effective method to enhance LLM translation capabilities.
                </p>
                <p class="text-lg leading-relaxed text-slate-700 dark:text-slate-300">
                    The approach works by identifying translation challenges (polysemous words, domain-specific terms, and cultural vocabulary) and incorporating semantic focus into structured prompts. This helps LLMs activate relevant knowledge from their vast data pool, resulting in more accurate translations without requiring additional model training or parallel data.
                </p>
            </div>
        </section>
        
        <!-- Abstract -->
        <section class="mb-8">
            <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">üî¨ Abstract</h2>
            <div class="section-content">
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    Large language models have demonstrated exceptional performance across cross-lingual NLP tasks, including machine translation. However, persistent challenges remain in addressing <span class="glossary-term relative border-b border-dotted border-slate-400 cursor-help" data-definition="Words with complex semantics or uncommon vocabulary that are highly sensitive to context">context-sensitive units (CSUs)</span>, such as polysemous words.
                </p>
                
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    These CSUs not only affect local translation accuracy but also impact LLMs' understanding capability for entire sentences and tasks, potentially leading to translation failure. To address this problem, we propose a simple but effective method to enhance LLMs' MT capabilities by acquiring CSUs and applying semantic focus.
                </p>
                
                <div class="bg-green-50 dark:bg-green-950 border-l-4 border-green-400 dark:border-green-500 p-4 rounded-r-lg mb-4">
                    <p class="text-sm text-green-800 dark:text-green-200">
                        <strong>Key Innovation:</strong> We dynamically analyze and identify translation challenges, then incorporate them into LLMs in a structured manner to mitigate mistranslations caused by information flattening.
                    </p>
                </div>
                
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    Our method efficiently activates LLMs to identify and apply relevant knowledge from their vast data pool, ensuring more accurate translations for difficult terms. On benchmark datasets, our proposed method achieved competitive performance compared to multiple existing open-source MT baseline models, demonstrating effectiveness across both similar and distant language pairs.
                </p>
                
                <div class="bg-blue-50 dark:bg-blue-950 border-l-4 border-blue-400 dark:border-blue-500 p-4 rounded-r-lg">
                    <p class="text-sm text-blue-800 dark:text-blue-200">
                        <strong>Practical Impact:</strong> The method requires no additional model training and enhances LLMs' performance across multiple NLP tasks with minimal resource consumption.
                    </p>
                </div>
            </div>
        </section>

        <!-- Introduction -->
        <section class="mb-8">
            <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">1. Introduction</h2>
            <div class="section-content">
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    The rapid development of large language models has revolutionized cross-lingual NLP tasks, particularly in machine translation. Current LLM-based translation paradigms primarily employ two approaches: <strong>prompt engineering</strong> and <strong>instruction fine-tuning</strong>.
                </p>
                
                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">The Challenge: Semantic Ambiguity</h3>
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    While these methods showcase impressive capabilities, several persistent challenges remain critical: systematic inaccuracies in translating certain context-sensitive units, such as polysemous words. These CSUs can impair the model's understanding of entire sentences and may even lead to catastrophic failures.
                </p>
                
                <div class="bg-amber-50 dark:bg-amber-950 border-l-4 border-amber-400 dark:border-amber-500 p-4 rounded-r-lg mb-4">
                    <p class="text-sm text-amber-800 dark:text-amber-200">
                        <strong>Example Problem:</strong> The English word "bank" has multiple translations in Chinese (Ê≤≥Â≤∏ for river bank, Èì∂Ë°å for financial institution), leading to confusion in LLMs' translation process.
                    </p>
                </div>
                
                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Root Cause Analysis</h3>
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    The key issue lies in the bottleneck of LLMs' knowledge utilization for CSUs. Most LLM-based MT methods treat entire sentences as homogeneous units, neglecting the varying context-sensitive constituent words that lead to varying translation difficulty.
                </p>
                
                <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
                    <li>Sentences contain words with varying degrees of translation difficulty</li>
                    <li>Polysemous words require precise contextual understanding</li>
                    <li>Accurate translation requires precise filtering and reorganization of vast pre-trained knowledge</li>
                    <li>Extracting relevant knowledge from massive knowledge bases is challenging</li>
                </ul>
                
                <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Our Solution: Dynamic Focus Anchoring</h3>
                <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                    We propose a resource-efficient method that integrates semantic focus into dynamic structured prompts to address semantic ambiguity caused by CSUs. By explicitly applying semantic focus to CSUs, we overcome the knowledge extraction bottleneck of LLMs.
                </p>
                
                                 <div class="bg-purple-50 dark:bg-purple-950 border-l-4 border-purple-400 dark:border-purple-500 p-4 rounded-r-lg">
                     <p class="text-sm text-purple-800 dark:text-purple-200">
                         <strong>Two-Stage Approach:</strong> 1) CSU identification and classification, 2) Hierarchical semantic constraint injection through structured prompts.
                     </p>
                 </div>
             </div>
         </section>

         <!-- Preliminary Experiment -->
         <section class="mb-8">
             <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">2. Preliminary Experiment</h2>
             <div class="section-content">
                 <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                     To demonstrate the impact of CSUs on LLM translation, we conducted experiments with two English sentences containing polysemous words that have multiple corresponding translations in Chinese.
                 </p>
                 
                 <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Experimental Setup</h3>
                 <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                     Using a classic prompt template ("Translate the following sentence to Chinese:"), we tested baseline Llama3-8b against our CSU-augmented approach that explicitly highlights challenging words.
                 </p>
                 
                 <div class="bg-red-50 dark:bg-red-950 border-l-4 border-red-400 dark:border-red-500 p-4 rounded-r-lg mb-4">
                     <p class="text-sm text-red-800 dark:text-red-200 mb-2">
                         <strong>Case Study 1:</strong> "The bank can be very dangerous this time of year."
                     </p>
                     <ul class="text-sm text-red-800 dark:text-red-200 space-y-1 ml-4">
                         <li><strong>Baseline:</strong> ‰ªäÂπ¥Ëøô‰∏™Êó∂ÂÄôÔºåÈì∂Ë°åÂæàÂç±Èô©„ÄÇ (Incorrect - translated as financial bank)</li>
                         <li><strong>CSU-augmented:</strong> ‰ªäÂπ¥Ëøô‰∏™Êó∂ÂÄôÔºåÂéªÊ≤≥ËæπÁöÑÂç±Èô©ÊÄßÈùûÂ∏∏È´ò„ÄÇ (Correct - translated as river bank)</li>
                     </ul>
                 </div>
                 
                 <div class="bg-orange-50 dark:bg-orange-950 border-l-4 border-orange-400 dark:border-orange-500 p-4 rounded-r-lg mb-4">
                     <p class="text-sm text-orange-800 dark:text-orange-200 mb-2">
                         <strong>Case Study 2:</strong> "Students must choose their subjects for next year."
                     </p>
                     <ul class="text-sm text-orange-800 dark:text-orange-200 space-y-1 ml-4">
                         <li><strong>Baseline:</strong> Translation failed - provided non-task response</li>
                         <li><strong>CSU-augmented:</strong> Successful translation after highlighting CSUs</li>
                     </ul>
                 </div>
                 
                 <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Key Findings</h3>
                 <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
                     <li>Highlighting challenging words significantly improves translation accuracy</li>
                     <li>CSUs influence model understanding of entire sentences, potentially causing translation failure</li>
                     <li>Simple semantic focus guidance is more effective than providing multiple translation options</li>
                     <li>Too much information can lead to counterproductive results (model includes all possible translations)</li>
                 </ul>
                 
                 <div class="bg-green-50 dark:bg-green-950 border-l-4 border-green-400 dark:border-green-500 p-4 rounded-r-lg">
                     <p class="text-sm text-green-800 dark:text-green-200">
                         <strong>Critical Insight:</strong> The key is stimulating the model's ability for independent analysis and thoughtful reasoning, rather than providing reference answers.
                     </p>
                 </div>
             </div>
         </section>

         <!-- Methodology -->
         <section class="mb-8">
             <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">3. Methodology: Dynamic Focus Anchoring (DFA)</h2>
             <div class="section-content">
                 <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                     Based on our analysis of <span class="glossary-term relative border-b border-dotted border-slate-400 cursor-help" data-definition="Problem where LLMs fail to grasp the true meaning of CSUs in context, leading to incorrect translations">semantic ambiguity</span>, we propose Dynamic Focus Anchoring (DFA) - a simple yet effective method to enhance LLMs' MT by explicitly indicating CSUs to provide meta-cognitive guidance.
                 </p>
                 
                 <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">3.1 Semantic Confusion Problem</h3>
                 <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                     Current prompt-based MT paradigms exhibit critical brittleness when processing sentences containing CSUs. We define CSUs as words with complex semantics that are highly sensitive to context, presenting semantic distinctions with wide gaps as context varies.
                 </p>
                 
                 <div class="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 mb-4">
                     <p class="text-sm text-slate-600 dark:text-slate-400 mb-2"><strong>Mathematical Definition:</strong></p>
                     <p class="text-sm text-slate-600 dark:text-slate-400 font-mono">CSU = {w ‚àà W | ContextDependent(w)}</p>
                     <p class="text-sm text-slate-600 dark:text-slate-400 font-mono">S(w, C‚ÇÅ) ‚â† S(w, C‚ÇÇ) ... ‚â† S(w, C‚Çô)</p>
                     <p class="text-sm text-slate-600 dark:text-slate-400 mt-2">Where C represents context and S represents word semantics.</p>
                 </div>
                 
                 <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">3.2 DFA Method Framework</h3>
                 <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                     DFA contains two main steps: <strong>CSU identification</strong> and <strong>semantic focus injection</strong>. We develop a dual-layer semantic exploration mechanism combining external knowledge guidance with internal knowledge activation.
                 </p>
                 
                 <h4 class="text-base font-semibold mb-2 text-slate-800 dark:text-slate-200">Step 1: CSU Identification</h4>
                 <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                     We consider three core types of CSUs:
                 </p>
                 <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
                     <li><strong><span class="glossary-term relative border-b border-dotted border-slate-400 cursor-help" data-definition="Words with multiple meanings that vary significantly across different contexts">Polysemous CSUs</span>:</strong> Identified using multilingual lexicons (MUSE dataset) with semantic filtering</li>
                     <li><strong><span class="glossary-term relative border-b border-dotted border-slate-400 cursor-help" data-definition="Technical vocabulary specific to particular fields or domains">Domain-specific CSUs</span>:</strong> Extracted through internal knowledge activation</li>
                     <li><strong><span class="glossary-term relative border-b border-dotted border-slate-400 cursor-help" data-definition="Culturally-specific vocabulary requiring contextual cultural understanding">Cultural CSUs</span>:</strong> Identified through structured prompt templates</li>
                 </ul>
                 
                 <h4 class="text-base font-semibold mb-2 text-slate-800 dark:text-slate-200">Step 2: Semantic Focus Injection</h4>
                 <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                     After acquiring CSUs, we integrate them into the foundational MT prompt using the formula:
                 </p>
                 
                 <div class="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 mb-4">
                     <p class="text-sm text-slate-600 dark:text-slate-400 font-mono">I_enhanced = I_base ‚äï W_CSUs</p>
                     <p class="text-sm text-slate-600 dark:text-slate-400 mt-2">Where ‚äï represents focus injection enhancement operation</p>
                 </div>
                 
                 <div class="bg-blue-50 dark:bg-blue-950 border-l-4 border-blue-400 dark:border-blue-500 p-4 rounded-r-lg">
                     <p class="text-sm text-blue-800 dark:text-blue-200">
                         <strong>Design Priorities:</strong> 1) Initiative guidance without providing reference answers, 2) Knowledge resource optimization by focusing on key words, 3) Prompt conciseness (maximum k=8 challenging words).
                     </p>
                 </div>
                          </div>
         </section>

         <!-- Results and Key Findings -->
         <section class="mb-8">
             <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">4. Results and Key Findings</h2>
             <div class="section-content">
                 <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                     We evaluated DFA on <span class="glossary-term relative border-b border-dotted border-slate-400 cursor-help" data-definition="Workshop on Machine Translation 2022 benchmark dataset covering news, social, e-commerce, and conversation domains">WMT22 test set</span> using both similar language pairs (English-German) and distant language pairs (English-Chinese), with Llama2-7b and Llama3-8b as backbone models.
                 </p>
                 
                 <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Main Results</h3>
                 <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                     DFA showed consistent improvements across all language pairs and metrics, with an average improvement of <strong>0.83 in COMET scores</strong> and <strong>0.81 in BLEU scores</strong> compared to the strongest baseline (Bayling2).
                 </p>
                 
                 <div class="bg-green-50 dark:bg-green-950 border-l-4 border-green-400 dark:border-green-500 p-4 rounded-r-lg mb-4">
                     <p class="text-sm text-green-800 dark:text-green-200 mb-2">
                         <strong>Performance Highlights:</strong>
                     </p>
                     <ul class="text-sm text-green-800 dark:text-green-200 space-y-1 ml-4">
                         <li><strong>EN-DE:</strong> +1.11 COMET, +0.22 BLEU (Llama2 backbone)</li>
                         <li><strong>EN-ZH:</strong> +0.3 COMET, +0.35 BLEU (Llama2 backbone)</li>
                         <li><strong>Consistent improvement across both similar and distant language pairs</strong></li>
                     </ul>
                 </div>
                 
                 <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Ablation Study Results</h3>
                 <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                     Each type of CSU contributed to performance improvement:
                 </p>
                 <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
                     <li><strong>Polysemous CSUs:</strong> Most significant impact on accuracy</li>
                     <li><strong>Domain-specific terms:</strong> Improved technical vocabulary handling</li>
                     <li><strong>Cultural CSUs:</strong> Enhanced cross-cultural understanding</li>
                     <li><strong>Semantic filtering:</strong> Outperformed simple polysemy detection</li>
                 </ul>
                 
                 <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Practical Case Studies</h3>
                 <div class="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 mb-4">
                     <p class="text-sm text-slate-600 dark:text-slate-400 mb-2"><strong>Case 1 - Polysemous Translation:</strong></p>
                     <p class="text-sm text-slate-600 dark:text-slate-400">Source: "Go to your Home screen."</p>
                     <p class="text-sm text-slate-600 dark:text-slate-400">Baseline: Translation failure</p>
                     <p class="text-sm text-slate-600 dark:text-slate-400">DFA: "Gehen Sie zu Ihrem Startbildschirm." ‚úì</p>
                 </div>
                 
                 <div class="bg-blue-50 dark:bg-blue-950 border-l-4 border-blue-400 dark:border-blue-500 p-4 rounded-r-lg">
                     <p class="text-sm text-blue-800 dark:text-blue-200">
                         <strong>Key Achievement:</strong> DFA requires no model fine-tuning and works across multiple language pairs with minimal resource consumption, making it highly practical for real-world deployment.
                     </p>
                 </div>
             </div>
         </section>

         <!-- Conclusion -->
         <section class="mb-8">
             <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">5. Conclusion</h2>
             <div class="section-content">
                 <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                     This study introduces Dynamic Focus Anchoring (DFA), an effective method to enhance LLM machine translation capabilities by addressing semantic confusion through dynamic focus on Context-Sensitive Units.
                 </p>
                 
                 <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Key Contributions</h3>
                 <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
                     <li>First investigation of CSU impact on LLM cross-lingual translation and identification of "semantic ambiguity" problem</li>
                     <li>Novel resource-efficient method incorporating semantic focus into dynamically structured prompts</li>
                     <li>Demonstrated significant translation accuracy improvements across various language pairs</li>
                     <li>No additional model training required - works with existing LLMs</li>
                 </ul>
                 
                 <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Practical Impact</h3>
                 <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
                     DFA helps LLMs accurately retrieve knowledge related to translation focus and incorporate it into the translation process, leading to high-quality translations without model training. The method's effectiveness extends beyond machine translation to potentially benefit other NLP tasks involving semantic ambiguity.
                 </p>
                 
                 <div class="bg-purple-50 dark:bg-purple-950 border-l-4 border-purple-400 dark:border-purple-500 p-4 rounded-r-lg">
                     <p class="text-sm text-purple-800 dark:text-purple-200">
                         <strong>Future Directions:</strong> The approach can be extended to analyze additional types of challenging vocabulary and applied to other NLP tasks like dialogue systems where semantic confusion exists.
                     </p>
                 </div>
             </div>
         </section>
         
                  <!-- Glossary -->
         <section class="mb-8">
             <h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">üìö Glossary</h2>
             <div class="section-content">
                 <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                     <div class="bg-slate-50 dark:bg-slate-800 p-3 rounded-lg border border-slate-200 dark:border-slate-700">
                         <h4 class="font-semibold text-slate-800 dark:text-slate-200">CSU (Context-Sensitive Units)</h4>
                         <p class="text-sm text-slate-600 dark:text-slate-400">Words with complex semantics or uncommon vocabulary that are highly sensitive to context, such as polysemous words, domain-specific terms, and cultural vocabulary.</p>
                     </div>
                     
                     <div class="bg-slate-50 dark:bg-slate-800 p-3 rounded-lg border border-slate-200 dark:border-slate-700">
                         <h4 class="font-semibold text-slate-800 dark:text-slate-200">DFA (Dynamic Focus Anchoring)</h4>
                         <p class="text-sm text-slate-600 dark:text-slate-400">A resource-efficient method that enhances LLM translation by dynamically identifying challenging vocabulary and incorporating semantic focus into structured prompts.</p>
                     </div>
                     
                     <div class="bg-slate-50 dark:bg-slate-800 p-3 rounded-lg border border-slate-200 dark:border-slate-700">
                         <h4 class="font-semibold text-slate-800 dark:text-slate-200">Semantic Ambiguity</h4>
                         <p class="text-sm text-slate-600 dark:text-slate-400">Problem where LLMs fail to grasp the true meaning of CSUs in context, leading to incorrect translations or complete translation failure.</p>
                     </div>
                     
                     <div class="bg-slate-50 dark:bg-slate-800 p-3 rounded-lg border border-slate-200 dark:border-slate-700">
                         <h4 class="font-semibold text-slate-800 dark:text-slate-200">Polysemous Words</h4>
                         <p class="text-sm text-slate-600 dark:text-slate-400">Words with multiple meanings that vary significantly across different contexts (e.g., "bank" can mean financial institution or river bank).</p>
                     </div>
                     
                     <div class="bg-slate-50 dark:bg-slate-800 p-3 rounded-lg border border-slate-200 dark:border-slate-700">
                         <h4 class="font-semibold text-slate-800 dark:text-slate-200">Domain-specific Terms</h4>
                         <p class="text-sm text-slate-600 dark:text-slate-400">Technical vocabulary specific to particular fields or domains that require specialized knowledge for accurate translation.</p>
                     </div>
                     
                     <div class="bg-slate-50 dark:bg-slate-800 p-3 rounded-lg border border-slate-200 dark:border-slate-700">
                         <h4 class="font-semibold text-slate-800 dark:text-slate-200">Cultural CSUs</h4>
                         <p class="text-sm text-slate-600 dark:text-slate-400">Culturally-specific vocabulary requiring contextual cultural understanding for proper translation across different linguistic communities.</p>
                     </div>
                     
                     <div class="bg-slate-50 dark:bg-slate-800 p-3 rounded-lg border border-slate-200 dark:border-slate-700">
                         <h4 class="font-semibold text-slate-800 dark:text-slate-200">BLEU Score</h4>
                         <p class="text-sm text-slate-600 dark:text-slate-400">Bilingual Evaluation Understudy - a metric for automatically evaluating machine translation quality by comparing n-gram overlap with reference translations.</p>
                     </div>
                     
                     <div class="bg-slate-50 dark:bg-slate-800 p-3 rounded-lg border border-slate-200 dark:border-slate-700">
                         <h4 class="font-semibold text-slate-800 dark:text-slate-200">COMET Score</h4>
                         <p class="text-sm text-slate-600 dark:text-slate-400">Crosslingual Optimized Metric for Evaluation of Translation - a neural-based metric that better correlates with human judgment for translation quality assessment.</p>
                     </div>
                     
                     <div class="bg-slate-50 dark:bg-slate-800 p-3 rounded-lg border border-slate-200 dark:border-slate-700">
                         <h4 class="font-semibold text-slate-800 dark:text-slate-200">WMT</h4>
                         <p class="text-sm text-slate-600 dark:text-slate-400">Workshop on Machine Translation - an annual shared task and conference providing benchmark datasets for machine translation evaluation.</p>
                     </div>
                     
                     <div class="bg-slate-50 dark:bg-slate-800 p-3 rounded-lg border border-slate-200 dark:border-slate-700">
                         <h4 class="font-semibold text-slate-800 dark:text-slate-200">MUSE</h4>
                         <p class="text-sm text-slate-600 dark:text-slate-400">Multilingual Unsupervised and Supervised Embeddings - a library providing multilingual word embeddings and bilingual dictionaries for 110+ languages.</p>
                     </div>
                     
                     <div class="bg-slate-50 dark:bg-slate-800 p-3 rounded-lg border border-slate-200 dark:border-slate-700">
                         <h4 class="font-semibold text-slate-800 dark:text-slate-200">Semantic Focus Injection</h4>
                         <p class="text-sm text-slate-600 dark:text-slate-400">The process of integrating identified CSUs into MT prompts to explicitly guide LLMs toward focusing on challenging vocabulary during translation.</p>
                     </div>
                     
                     <div class="bg-slate-50 dark:bg-slate-800 p-3 rounded-lg border border-slate-200 dark:border-slate-700">
                         <h4 class="font-semibold text-slate-800 dark:text-slate-200">Knowledge Extraction Bottleneck</h4>
                         <p class="text-sm text-slate-600 dark:text-slate-400">The challenge LLMs face in precisely filtering and reorganizing relevant knowledge from their vast pre-trained knowledge base for specific translation tasks.</p>
                     </div>
                 </div>
             </div>
         </section>
        
    </main>
    
    <script src="../script.js"></script>
</body>
</html> 