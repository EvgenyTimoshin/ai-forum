<!-- DeepSeek-R1 Section -->
<h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">DeepSeek-R1: Open-Source Reasoning at Scale</h2>
<div class="section-content">
    <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
        DeepSeek-R1, released in early 2025, is a fully open-source reasoning model trained with large-scale reinforcement learning. It is designed to rival proprietary models in math, code, and reasoning, and is available for both research and commercial use<a href="#source7" class="no-audio text-blue-600 dark:text-blue-400 hover:underline">[7]</a>.
    </p>
    <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Architecture & Capabilities</h3>
    <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
        <li>Mixture-of-Experts (MoE) transformer, 671B total parameters (37B active per token)</li>
        <li>128K context window</li>
        <li>Trained via RL, with open weights and technical report</li>
        <li>Supports tool use, function calling, and chain-of-thought reasoning</li>
    </ul>
    <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Benchmarks & Performance</h3>
    <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
        <li>90.8% MMLU, 79.8% AIME 2024, 65.9% LiveCodeBench, 49.2% SWE-bench<a href="#source7" class="no-audio text-blue-600 dark:text-blue-400 hover:underline">[7]</a></li>
        <li>Comparable to OpenAI o1-mini and other top-tier models on math, code, and reasoning</li>
        <li>Open weights allow for fine-tuning and research</li>
    </ul>
    <div class="bg-blue-50 dark:bg-blue-950 border-l-4 border-blue-400 dark:border-blue-500 p-4 rounded-r-lg mb-4">
        <p class="text-sm text-blue-800 dark:text-blue-200">
            <strong>Key Insight:</strong> DeepSeek-R1 is the leading open-source alternative to proprietary LLMs for advanced reasoning, math, and code. Its open weights and strong performance make it ideal for research, customization, and cost-sensitive deployments.
        </p>
    </div>
    <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Pricing & Availability</h3>
    <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
        <li>Open-source MIT license, free for research and commercial use</li>
        <li>API access via DeepSeek, HuggingFace, and community platforms</li>
    </ul>
    <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Strengths & Weaknesses</h3>
    <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
        <li>Strengths: open weights, strong math/code/reasoning, customizable, cost-free for self-hosting</li>
        <li>Weaknesses: smaller context than Gemini/GPT-4.1, not as strong as Opus 4 or Gemini 2.5 Pro on some agentic tasks, requires significant compute for self-hosting</li>
    </ul>
    <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Best Use Cases</h3>
    <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
        <li>Research, education, custom agentic workflows, privacy-sensitive deployments, and organizations seeking to avoid vendor lock-in</li>
    </ul>
    <div class="bg-amber-50 dark:bg-amber-950 border-l-4 border-amber-400 dark:border-amber-500 p-4 rounded-r-lg">
        <p class="text-sm text-amber-800 dark:text-amber-200">
            <strong>Note:</strong> DeepSeek-R1 is the best choice for open, transparent, and customizable LLM deployments<a href="#source7" class="no-audio text-blue-600 dark:text-blue-400 hover:underline">[7]</a>.
        </p>
    </div>
</div> 