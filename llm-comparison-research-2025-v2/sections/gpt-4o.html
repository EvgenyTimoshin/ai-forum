<!-- GPT-4o Section -->
<h2 class="section-header text-2xl font-bold mb-4 text-slate-800 dark:text-slate-200 cursor-pointer hover:text-blue-600 dark:hover:text-blue-400 select-none">GPT-4o: OpenAI's Omnimodal Model</h2>
<div class="section-content">
    <p class="text-base leading-relaxed text-slate-700 dark:text-slate-300 mb-4">
        GPT-4o, released by OpenAI in 2024, is a groundbreaking <span class="glossary-term relative border-b border-dotted border-slate-400 cursor-help" data-definition="A model that can process and generate text, audio, image, and video in a unified architecture.">omnimodal</span> LLM. It is trained end-to-end across text, vision, and audio, and is designed for real-time, multimodal agentic workflows<a href="#source3" class="no-audio text-blue-600 dark:text-blue-400 hover:underline">[3]</a>.
    </p>
    <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Architecture & Capabilities</h3>
    <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
        <li>Omnimodal: accepts text, audio, image, and video input; generates text, audio, and image output</li>
        <li>128K context window</li>
        <li>Fast response time (as low as 232ms for audio)</li>
        <li>API and ChatGPT access</li>
    </ul>
    <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Benchmarks & Performance</h3>
    <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
        <li>90.2% MMLU, 54.6% SWE-bench, 74.8% MMMU, 48.1% AIME 2024<a href="#source3" class="no-audio text-blue-600 dark:text-blue-400 hover:underline">[3]</a></li>
        <li>Matches GPT-4 Turbo on text/coding, outperforms on vision/audio, and is 50% cheaper</li>
        <li>Strong multilingual and vision capabilities</li>
    </ul>
    <div class="bg-blue-50 dark:bg-blue-950 border-l-4 border-blue-400 dark:border-blue-500 p-4 rounded-r-lg mb-4">
        <p class="text-sm text-blue-800 dark:text-blue-200">
            <strong>Key Insight:</strong> GPT-4o is the most versatile model for multimodal agentic workflows, especially where real-time audio, vision, and text are required. It is a top choice for interactive assistants, voice agents, and applications needing fast, cross-modal reasoning.
        </p>
    </div>
    <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Pricing & Availability</h3>
    <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
        <li>$5 per million input tokens, $15 per million output tokens (API)</li>
        <li>Available via OpenAI API and ChatGPT</li>
    </ul>
    <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Strengths & Weaknesses</h3>
    <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
        <li>Strengths: real-time multimodal reasoning, fast response, strong vision/audio, cost-effective for multimodal tasks</li>
        <li>Weaknesses: smaller context window than some competitors, not always the best for pure text/coding at the highest level</li>
    </ul>
    <h3 class="text-lg font-semibold mb-3 text-slate-800 dark:text-slate-200">Best Use Cases</h3>
    <ul class="list-disc list-inside space-y-2 text-slate-700 dark:text-slate-300 ml-4 mb-4">
        <li>Voice assistants, real-time chatbots, multimodal research, accessibility tools, and interactive agentic systems</li>
    </ul>
    <div class="bg-amber-50 dark:bg-amber-950 border-l-4 border-amber-400 dark:border-amber-500 p-4 rounded-r-lg">
        <p class="text-sm text-amber-800 dark:text-amber-200">
            <strong>Note:</strong> GPT-4o is especially recommended for applications that require seamless integration of text, audio, and vision in real time<a href="#source3" class="no-audio text-blue-600 dark:text-blue-400 hover:underline">[3]</a>.
        </p>
    </div>
</div> 